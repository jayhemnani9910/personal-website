---
title: "Smart Water Management (IoT)"
summary: "Raspberry Pi + sensors for usage tracking and saving alerts."
role: "Developer"
tags: ["iot", "raspberry-pi"]
tech: ["Raspberry Pi", "Python", "IoT sensors"]
challenge: "Detect anomalies and reduce wastage."
solution:
  - "Sensor data ingestion to local hub; thresholds + rolling averages."
  - "Alerts + usage summaries; optional cloud sync."
impact:
  - "Data-driven water saving & maintenance scheduling."
deepDive:
  context: |
    Water scarcity affects over 2 billion people globally, and residential water waste accounts for 30-50% of total consumption in many regions. This project was born from observing how difficult it is for households to track water usage patterns and detect leaks before they become costly problems.

    Traditional water meters only provide monthly aggregate data, making it impossible to identify specific wastage sources or unusual consumption patterns. By deploying real-time IoT monitoring at key water access points, we can shift from reactive billing to proactive conservation, potentially reducing household water waste by 15-30%.

  architecture: |
    The system follows a distributed edge-to-cloud architecture:

    **Edge Layer (Sensors):**
    - Flow sensors installed at main water line and high-usage fixtures (shower, washing machine, outdoor faucets)
    - YF-S201 Hall-effect flow sensors measuring 1-30 L/min with pulse frequency output
    - Each sensor connected via GPIO to local microcontroller nodes

    **Gateway Layer (Raspberry Pi Hub):**
    - Raspberry Pi 3B+ acts as central data aggregator
    - Collects pulse data from multiple sensor nodes via I2C and GPIO
    - Performs edge analytics: flow rate calculation, anomaly detection, local alerting
    - Buffers data locally with SQLite for offline resilience

    **Cloud Layer (Optional):**
    - MQTT broker for telemetry ingestion (uses Eclipse Mosquitto)
    - Time-series database (InfluxDB) for historical storage
    - Web dashboard for visualization and remote monitoring

    **Data Flow:**
    Sensors (pulse signals) → GPIO interrupts → Python service → Local processing → SQLite cache → MQTT publish → Cloud storage → Dashboard API

  components:
    - name: "YF-S201 Flow Sensors"
      description: "Hall-effect water flow sensors providing pulse output proportional to flow rate. Calibrated to ~4.5 pulses per mL. Installed inline at 1/2\" pipe connections."

    - name: "Raspberry Pi 3B+ Hub"
      description: "Central processing unit running Raspbian OS. Handles GPIO interrupt processing, data aggregation, local analytics, and cloud synchronization."

    - name: "Python Data Pipeline"
      description: "Custom daemon service using RPi.GPIO library for interrupt handling, pandas for data processing, and paho-mqtt for cloud communication."

    - name: "SQLite Local Storage"
      description: "Embedded database for buffering readings during network outages and maintaining 30-day local history for offline queries."

    - name: "MQTT Broker"
      description: "Eclipse Mosquitto broker deployed on DigitalOcean. Handles pub/sub messaging between edge devices and cloud analytics services."

    - name: "InfluxDB Time-Series DB"
      description: "Cloud database optimized for time-stamped sensor data. Stores flow measurements with millisecond precision for historical analysis."

    - name: "Grafana Dashboard"
      description: "Web-based visualization platform showing real-time flow rates, daily consumption trends, leak alerts, and comparative analytics."

  dataFlow:
    - step: "Sensor Reading"
      detail: "Flow sensor generates pulse train as water passes through turbine. Each pulse represents ~0.22 mL of water flow."

    - step: "Interrupt Handling"
      detail: "Raspberry Pi GPIO configured with edge detection interrupts (RISING). Python callback increments pulse counter with microsecond timestamps."

    - step: "Flow Rate Calculation"
      detail: "Every 1-second interval, pulse count converted to flow rate: Flow (L/min) = (pulses × 60) / (calibration_factor × seconds). Calibration factor determined through empirical testing."

    - step: "Anomaly Detection"
      detail: "Rolling 7-day baseline established for each hour of day. Current flow compared against baseline ± 2 standard deviations. Continuous flow for >30 minutes triggers leak alert."

    - step: "Local Storage"
      detail: "Readings batch-written to SQLite every 10 seconds with transaction batching for I/O efficiency. Schema: timestamp, sensor_id, flow_rate, total_volume, anomaly_flag."

    - step: "Cloud Synchronization"
      detail: "MQTT client publishes aggregated readings every 60 seconds to topic 'home/water/{sensor_id}'. QoS level 1 ensures at-least-once delivery."

    - step: "Alert Notification"
      detail: "When anomaly detected, system sends immediate alert via Pushover API to mobile devices. Includes sensor location, current vs. expected flow, and estimated waste rate."

  keyDecisions:
    - decision: "MQTT vs. HTTP for Telemetry"
      rationale: "Chose MQTT over REST API for lightweight protocol overhead, built-in QoS levels, and persistent session support. Critical for reliability on home Wi-Fi networks with intermittent connectivity."

    - decision: "Edge Analytics vs. Cloud-Only Processing"
      rationale: "Implemented anomaly detection on Raspberry Pi to enable offline alerting and reduce cloud bandwidth costs. Sending raw pulse data would consume 100x more bandwidth than aggregated minute-level summaries."

    - decision: "YF-S201 Sensor Selection"
      rationale: "Selected over ultrasonic sensors due to 10x lower cost ($5 vs. $50), sufficient accuracy for residential use (±5%), and simple GPIO integration. Trade-off: requires inline installation vs. non-invasive clamp-on."

    - decision: "SQLite vs. No Local Storage"
      rationale: "Added SQLite buffer after experiencing data loss during internet outages. Enables local dashboard for privacy-conscious users and reduces cloud storage costs by 40% through intelligent aggregation before upload."

    - decision: "Calibration Methodology"
      rationale: "Performed field calibration by measuring actual volume dispensed vs. pulse counts across different flow rates (1-20 L/min). Found factory calibration off by 8%, requiring custom correction factor per sensor unit."

  codeSnippets:
    - title: "GPIO Interrupt Handler for Flow Pulses"
      language: "python"
      code: |
        import RPi.GPIO as GPIO
        from datetime import datetime

        class FlowSensor:
            def __init__(self, pin, calibration_factor=4.5):
                self.pin = pin
                self.calibration_factor = calibration_factor
                self.pulse_count = 0
                self.flow_rate = 0.0
                self.total_volume = 0.0

                GPIO.setmode(GPIO.BCM)
                GPIO.setup(self.pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)
                GPIO.add_event_detect(self.pin, GPIO.RISING,
                                     callback=self.count_pulse)

            def count_pulse(self, channel):
                """Interrupt callback - executes on every pulse"""
                self.pulse_count += 1

            def calculate_flow(self, interval_seconds=1.0):
                """Calculate flow rate from pulse count"""
                # Flow rate (L/min) = (pulses/second) * 60 / calibration
                self.flow_rate = (self.pulse_count / interval_seconds) * 60 / self.calibration_factor

                # Total volume in liters
                volume_increment = self.pulse_count / self.calibration_factor / 1000
                self.total_volume += volume_increment

                # Reset pulse counter
                self.pulse_count = 0

                return self.flow_rate, self.total_volume

    - title: "Anomaly Detection Algorithm"
      language: "python"
      code: |
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta

        class AnomalyDetector:
            def __init__(self, db_connection, lookback_days=7):
                self.db = db_connection
                self.lookback_days = lookback_days
                self.baseline = self._compute_baseline()

            def _compute_baseline(self):
                """Build hourly usage profile from historical data"""
                query = """
                    SELECT strftime('%H', timestamp) as hour,
                           AVG(flow_rate) as mean_flow,
                           STDEV(flow_rate) as std_flow
                    FROM readings
                    WHERE timestamp >= datetime('now', '-7 days')
                    GROUP BY hour
                """
                df = pd.read_sql(query, self.db)
                return df.set_index('hour')

            def detect_anomaly(self, current_flow, timestamp):
                """Compare current flow to baseline with 2-sigma threshold"""
                hour = timestamp.strftime('%H')

                if hour not in self.baseline.index:
                    return False, "Insufficient baseline data"

                expected_mean = self.baseline.loc[hour, 'mean_flow']
                expected_std = self.baseline.loc[hour, 'std_flow']

                # Z-score calculation
                z_score = (current_flow - expected_mean) / (expected_std + 1e-6)

                is_anomaly = abs(z_score) > 2.0

                if is_anomaly:
                    deviation = ((current_flow - expected_mean) / expected_mean) * 100
                    message = f"Flow {deviation:+.1f}% above normal for {hour}:00"
                    return True, message

                return False, "Normal"

    - title: "MQTT Publisher with Retry Logic"
      language: "python"
      code: |
        import paho.mqtt.client as mqtt
        import json
        import time
        from collections import deque

        class MQTTPublisher:
            def __init__(self, broker, port=1883, retry_queue_size=1000):
                self.client = mqtt.Client()
                self.broker = broker
                self.port = port
                self.connected = False
                self.retry_queue = deque(maxlen=retry_queue_size)

                self.client.on_connect = self._on_connect
                self.client.on_disconnect = self._on_disconnect

                self._connect()

            def _connect(self):
                try:
                    self.client.connect(self.broker, self.port, keepalive=60)
                    self.client.loop_start()
                except Exception as e:
                    print(f"Connection failed: {e}")

            def _on_connect(self, client, userdata, flags, rc):
                self.connected = True
                print(f"Connected to MQTT broker with code {rc}")
                self._flush_retry_queue()

            def _on_disconnect(self, client, userdata, rc):
                self.connected = False
                print(f"Disconnected from broker with code {rc}")

            def publish_reading(self, sensor_id, flow_rate, total_volume):
                """Publish sensor reading with QoS 1"""
                payload = {
                    "sensor_id": sensor_id,
                    "timestamp": time.time(),
                    "flow_rate": round(flow_rate, 2),
                    "total_volume": round(total_volume, 3)
                }

                topic = f"home/water/{sensor_id}"

                if self.connected:
                    result = self.client.publish(topic,
                                                 json.dumps(payload),
                                                 qos=1)
                    if result.rc != mqtt.MQTT_ERR_SUCCESS:
                        self.retry_queue.append((topic, payload))
                else:
                    # Queue for retry when connection restored
                    self.retry_queue.append((topic, payload))

            def _flush_retry_queue(self):
                """Send queued messages after reconnection"""
                while self.retry_queue:
                    topic, payload = self.retry_queue.popleft()
                    self.client.publish(topic, json.dumps(payload), qos=1)

  learnings:
    - lesson: "Hardware Calibration is Critical for IoT Accuracy"
      detail: "Factory calibration of YF-S201 sensors varied by 5-12% between units. Implemented per-sensor calibration by measuring known volumes (5L, 10L, 20L) at different flow rates. This improved accuracy from ±10% to ±2%, essential for leak detection sensitivity. Learned to always validate sensor specs empirically."

    - lesson: "Edge Computing Reduces Cloud Costs and Improves Reliability"
      detail: "Initial design sent raw pulse data to cloud (10 msgs/sec). At $0.40 per million messages, this would cost $100/month per household. Moving to edge aggregation reduced to 1 msg/min (99.8% reduction) while enabling offline operation. Key insight: push intelligence to the edge wherever possible."

    - lesson: "Time Synchronization is Harder Than Expected in IoT"
      detail: "Raspberry Pi lacks RTC, relies on NTP for time sync. During network outages, clock drift caused timestamp errors of up to 15 minutes, corrupting time-series analysis. Added DS3231 RTC module ($3) to maintain accurate timestamps offline. IoT systems need independent time sources."

    - lesson: "Real-World Sensors Require Robust Error Handling"
      detail: "Experienced false readings from electromagnetic interference (EMI) when washing machine motor started, causing pulse spikes. Implemented pulse rate validation (max physical flow = 30 L/min) and median filtering over 5-second windows to reject outliers. Sensors in noisy environments need defensive signal processing."

    - lesson: "User Trust Requires Transparency in Anomaly Detection"
      detail: "Early version had 40% false positive rate on leak alerts, eroding user trust. Added 'confidence level' to alerts showing baseline data, z-score, and historical comparison. Users could see the reasoning and adjust sensitivity. For IoT in homes, explainability > pure accuracy."

    - lesson: "Battery Backup is Essential for Critical IoT Infrastructure"
      detail: "Power outages caused system to miss leak detection during critical window (pipe burst during outage). Added UPS with 4-hour runtime and low-battery MQTT alert. For safety-critical IoT, assume primary power will fail and design accordingly."
---
# Overview
Raspberry Pi + sensors for usage tracking and saving alerts.